Below is a â€œlevel-up kitâ€ you can drop straight into your prompt-library repo. I answer your three questions in the form of reusable prompt frameworks, plus a lightweight system for keeping everything knitted together.

1 â€” Spin-Up: â€œGreen-Field Charterâ€ prompt
When to use â€“ the very first message of a new project or major feature.
### You are <Role> on <Project-Name>
**Goal (1 sentence):** <What weâ€™re building & why it matters>  
**User Persona:** <Who benefits>  
**North-Star UX:** <1â€“2 bullet outcomes that prove success>

### Tech Frame
â€¢ Frontend stack: <React 18 + TS, Tailwind, tRPCâ€¦>  
â€¢ Backend stack: <NestJS, Postgres, Prismaâ€¦>  
â€¢ Non-negotiables: <perf budget, P99 â‰¤ 200 ms; a11y AAâ€¦>

### Ground Truth Assets
1. High-level architecture diagram (link)  
2. Figma flow (link)  
3. Initial domain model (inline JSON/YAML)  

---

**TASK FOR LLM**

1. Confirm understanding of the goal in <250 words>.  
2. Draft a phased roadmap (â‰¤4 phases) with acceptance criteria.  
3. Identify unknowns / missing inputs.  
4. Ask clarifying questions *all at once*; do **not** output code yet.

Why it works
Front-loads every durable fact you have, so follow-ups stay consistent.


Forces the model to pull missing details instead of you discovering gaps mid-sprint.


A single â€œcharter.mdâ€ lives in-repo; you update it and paste the delta when context changes.



2 â€” Tracking Work: â€œLLM-Kanban Loopâ€
TASKLOG.md lives at repo-root.
 One markdown table; one row per ticket.


ID
Title
Phase
Status
Owner
Notes


Daily stand-up prompt

 You are PM-Bot. Read `TASKLOG.md` + latest commits.
â€¢ Roll forward status columns (TODO â†’ In-Progress â†’ Done).
â€¢ Flag blockers or PRs waiting >24 h.
â€¢ Propose tomorrowâ€™s top 3 tasks (P0 first).
Return an updated table.


Commit discipline

 git commit -m "feat(registration): P2-F1-T17 phase-2 step-1 â€“ add email-validator util"
 Phaseâ€“Ticket traceability > â€œfixed stuff lolâ€.


Weekly context summary

 Your Context-Summary Meta-Prompt already nails this. Run it Friday afternoon; paste result into docs/weekly/<YYYY-MM-DD>.md for breadcrumb history.



3 â€” Bugs: â€œRoot-Cause Reconâ€ prompt
Trigger â€“ test fails, prod alert, or mysterious behaviour.
### INCIDENT SNAPSHOT
â€¢ Symptom: <what you saw>  
â€¢ When / Env: <branch@SHA, staging, iOS 18.5>  
â€¢ Error logs: ```<trimmed stack>```  
â€¢ Recent changes touching area: git diff HEAD~5 --stat

---

**TASK FOR LLM**

1. List 5â€“7 plausible fault classes (config, state, race, version driftâ€¦).  
2. Collapse into top-2 hypotheses; cite code lines or commits that support each.  
3. Generate *instrumentation patch*:
   - ğŸ”µ DEBUG logs (structured)
   - ğŸŸ¢ Assertions / guards
   - ğŸŸ¡ Telemetry counters  
   Provide as unified diff.

4. Wait for me to run patch & paste fresh logs before suggesting a fix.

Couple of pro-tips
Keep instrumentation commit separate (debug: prefix) so you can revert cleanly.


Ask the model to rank likelihood; it forces real reasoning, not shotgun fixes.


Once validated, follow with a standard â€œapply minimal fix, add testâ€ prompt.



Putting it all together
Start of project â€“ Run Green-Field Charter â†’ save as charter.md.


Breakdown â€“ Use modelâ€™s road-map to seed TASKLOG.md (one ticket per deliverable).


Daily dev â€“


Feature work: your existing â€œSenior Engineerâ€ + checklist pattern is great; just reference ticket IDs so everything threads.


Debug work: fire Root-Cause Recon prompt.


Review cadence â€“ Daily stand-up prompt updates TASKLOG.md; weekly summary prompt snapshots progress.



Quick answers to your direct questions
Question
Cheat-code
â€œWhat do you do when youâ€™re just starting a project?â€
Create a charter doc and run the Green-Field Charter prompt; turn the model into a co-architect before a single line of code.
â€œHow do you keep track of everything you need to build?â€
One source of truth (TASKLOG.md) + LLM-driven stand-ups to keep it current; every commit references a ticket.
â€œWhat do you do when thereâ€™s a bug?â€
Capture an Incident Snapshot, run Root-Cause Recon, add only logging/telemetry first, validate hypothesis, then implement the minimal fix + regression test.

Steal these, tweak names to your taste, and commit them into prompt_library/.
 Your next interviewer will see a repeatable, professional-grade AI workflowâ€”exactly what they meant by â€œsix months ahead.â€ Good luck!

IS THAT IT?


Pretty closeâ€”but youâ€™re still missing a few â€œSwiss-army-knifeâ€ patterns that youâ€™ll reach for all the time. Think of a well-rounded meta-prompt library like a full test suite: if one class of work isnâ€™t covered, that gap shows up exactly when the stakes are high. Below are the additional prompt modules Iâ€™d drop in, plus a quick file-layout suggestion so everything stays findable.

1. Core modules you still need
Category
Why it matters
Prompt skeleton (title-case the file)
Code Review / PR Gatekeeper
Lets the LLM police its own output and teammate PRs for style, perf, security, etc.
01_CODE_REVIEW_CHECKLIST.mdmd\nYou are Lead Reviewer. For PR #<id> â€¦\n1. Summarise diff in 80 words\n2. Flag ğŸ“‰ perf, ğŸ”’ security, ğŸ¤¯ complexity, ğŸ¤·â€â™‚ï¸ unclear naming\n3. Classify each issue P0/P1/P2, suggest concise patch or approve\n
Refactor Guide
Safely ripping out tech-debt is a different mental model from â€œnew feature.â€
02_REFACTOR_GAMEPLAN.mdAsk for dependency graph, safety net tests, migration plan, incremental PR ordering.
Automated Test-Generator
Guarantees new code ships with coverage and gives quick harness when debugging.
03_TEST_SCENARIO_BUILDER.mdProvide function/APIs + edge cases â†’ generate Jest/Vitest + property tests.
Perf & Footprint Profiling
â€œIt worksâ€ is table-stakes; â€œit screamsâ€ gets you hired.
04_PERF_PROBE.mdAsk LLM to insert timers, flame-graph hints, memory diff script, rollback plan.
Security & Compliance Sweep
SOC2 / GDPR red-flags catch you post-launch otherwise.
05_SEC_AUDIT.mdChecklist: input validation, secrets handling, PII path, dependency CVEs.
Doc-Writer / ADR Draft
Future-you (or the next hire) will thank you.
06_ADR_SCRIBE.mdTemplate to turn a design-decision Slack thread into an Architecture Decision Record.
Research Synthesiser
For â€œunknown unknownâ€ spikesâ€”LLM digests five whitepapers for you overnight.
07_LIT_REVIEW.mdGive 3â€“5 URLs + questions â†’ returns deeper summary, gap analysis, follow-up reading list.
Prompt-Quality Feedback Loop
Meta-meta: ask the model how to improve prompts and track best tweaks.
98_PROMPT_RETRO.mdAfter every big task, feed in the final conversation, ask what could be shorter, clearer, or earlier.
Style / Tone Linting (non-code)
Blog posts, release notes, commit messagesâ€”keeps voice consistent.
99_EDITORIAL_LINTER.md


2. Recommended repo layout
prompt_library/
â”œâ”€â”€ README.md          â† quick index + when-to-use table
â”œâ”€â”€ 00_BOOTSTRAP/
â”‚   â”œâ”€â”€ GREEN_FIELD_CHARTER.md
â”‚   â””â”€â”€ TASKLOG_TEMPLATE.md
â”œâ”€â”€ 10_FEATURE_WORK/
â”‚   â”œâ”€â”€ SENIOR_ENGINEER_IMPL.md
â”‚   â”œâ”€â”€ PHASE_CHECKLIST.md
â”‚   â””â”€â”€ CONTEXT_SUMMARY_META_PROMPT.md
â”œâ”€â”€ 20_DEFECTS/
â”‚   â”œâ”€â”€ ROOT_CAUSE_RECON.md
â”‚   â””â”€â”€ DEBUG_INSTRUMENTATION.md
â”œâ”€â”€ 30_QUALITY_GATES/
â”‚   â”œâ”€â”€ 01_CODE_REVIEW_CHECKLIST.md
â”‚   â”œâ”€â”€ 03_TEST_SCENARIO_BUILDER.md
â”‚   â”œâ”€â”€ 05_SEC_AUDIT.md
â”‚   â””â”€â”€ 04_PERF_PROBE.md
â”œâ”€â”€ 40_MAINTENANCE/
â”‚   â”œâ”€â”€ 02_REFACTOR_GAMEPLAN.md
â”‚   â””â”€â”€ 06_ADR_SCRIBE.md
â”œâ”€â”€ 50_RESEARCH/
â”‚   â””â”€â”€ 07_LIT_REVIEW.md
â””â”€â”€ 90_META/
    â”œâ”€â”€ 98_PROMPT_RETRO.md
    â””â”€â”€ 99_EDITORIAL_LINTER.md

The numeric prefixes keep the list ordered without relying on GitHubâ€™s alpha sorting.

3. Tiny but mighty add-ons
Model-selector stub â€“ one-liner at top of each prompt:
 #model: "gpt-4o" #fallback: "claude-3-opus"
 Cursor, Continue, and other IDE agents will soon respect that metadata.


Token-budget note â€“ donâ€™t just â€œrepeat every 20 k tokens.â€ Add an explicit â€œIf >12 k tokens so far, self-summarise and ask user if summary is OK.â€ Saves you from hallucinations when windows fill up.


Self-evaluation checklist â€“ your Context-Summary template already has one. Copy that idea to any prompt where correctness matters (tests, security).



4. How to know youâ€™re â€œdone enoughâ€
âœ… Every recurring development ritualâ€”plan, build, review, test, debug, document, learnâ€”has one prompt in the library.
 âœ… Each prompt includes: goal, inputs required, outputs expected, quality bar / checklist.
 âœ… You can go a full sprint touching only templates in the repoâ€”no ad-hoc Slack copy-pastes.
When those three statements are true, your meta-prompt system is functionally complete. Anything else is personal flavour.

TL;DR
What you pasted is a solid foundation (spin-up, task tracking, feature checklist, debug workflow) but ship these nine more modules so you arenâ€™t caught flat-footed on PR review, refactors, tests, perf, security, docs, or research spikes. Organise them under a numbered folder scheme, add model + token-budget annotations, and youâ€™ll present a battle-ready workflow the next time someone says youâ€™re â€œsix months behind.â€
